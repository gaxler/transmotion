<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Stages of Motion Transfer &mdash; TransMotion  documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Training Signals" href="train_signal.html" />
    <link rel="prev" title="Welcome to TransMotion’s documentation!" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> TransMotion
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Stages of Motion Transfer</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#detect-key-points">Detect Key-points</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#what-are-those-k-and-n-over-there">What are those <span class="math notranslate nohighlight">\(K\)</span> and <span class="math notranslate nohighlight">\(N\)</span> over there??</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#turn-key-points-to-optical-flow">Turn Key-points to optical flow</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#tps">TPS</a></li>
<li class="toctree-l3"><a class="reference internal" href="#why-do-we-need-occlusion-masks">Why do we need occlusion masks?</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#fill-in-the-occluded-areas">Fill-in the occluded areas</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="train_signal.html">Training Signals</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html">Doc Strings</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">TransMotion</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Stages of Motion Transfer</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/intro.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="stages-of-motion-transfer">
<h1>Stages of Motion Transfer<a class="headerlink" href="#stages-of-motion-transfer" title="Permalink to this headline"></a></h1>
<p>Motion Transfer Models take a still image and a video and animate the
still image such that it mimics the motion of the video. We refer to the
still image as the <strong>source</strong> and to the video as <strong>driving</strong>.</p>
<p>Our particular model is <a class="reference external" href="https://arxiv.org/abs/2203.14367">Thin-Plate Spline motion transfer</a> <a class="footnote-reference brackets" href="#id3" id="id1">0</a>.
It has three major stage:</p>
<ol class="arabic simple">
<li><p><strong>Detect Key-points</strong> - Use a ResNet model to discover points of
interest in an image. We do this for the source image and for the
driving video.</p></li>
<li><p><strong>Turn Key-points to optical flow</strong> - Based on the difference between
source and driving key-points we infer an optical flow <a class="footnote-reference brackets" href="#id4" id="id2">1</a>. Some
parts of the driving image can’t be seen in the source image, so if
we also try and infer where those areas are (we call those occluded
areas)</p></li>
<li><p><strong>Fill-in the occluded areas</strong> - A generative model is use to predict
what should go in the missing parts.</p></li>
</ol>
<dl class="footnote brackets">
<dt class="label" id="id3"><span class="brackets"><a class="fn-backref" href="#id1">0</a></span></dt>
<dd><p><a class="reference external" href="https://github.com/yoyo-nb/Thin-Plate-Spline-Motion-Model">original repository</a></p>
</dd>
<dt class="label" id="id4"><span class="brackets"><a class="fn-backref" href="#id2">1</a></span></dt>
<dd><p>where each pixel from the source image should move if we want to
deform it to match the driving image</p>
</dd>
</dl>
<section id="detect-key-points">
<h2>Detect Key-points<a class="headerlink" href="#detect-key-points" title="Permalink to this headline"></a></h2>
<p>We use a <code class="docutils literal notranslate"><span class="pre">ResNet18</span></code> model to predict the key-points. Key-points
are just 2D coordinates on an image. So the ResNet outputs a vector with
a dimension of <span class="math notranslate nohighlight">\(K \times N \times 2\)</span> and values <span class="math notranslate nohighlight">\(\in [0,1]\)</span>. <a class="reference internal" href="transmotion.html#transmotion.kp_detection.KPDetector" title="transmotion.kp_detection.KPDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">transmotion.kp_detection.KPDetector</span></code></a> is the key-points detector network and <a class="reference internal" href="transmotion.html#transmotion.kp_detection.KPResult" title="transmotion.kp_detection.KPResult"><code class="xref py py-class docutils literal notranslate"><span class="pre">transmotion.kp_detection.KPResult</span></code></a> bundles the outputs</p>
<section id="what-are-those-k-and-n-over-there">
<h3>What are those <span class="math notranslate nohighlight">\(K\)</span> and <span class="math notranslate nohighlight">\(N\)</span> over there??<a class="headerlink" href="#what-are-those-k-and-n-over-there" title="Permalink to this headline"></a></h3>
<p>The model uses multiple deformations (later on that) in this case <span class="math notranslate nohighlight">\(K\)</span> is the number of deformation and <span class="math notranslate nohighlight">\(N\)</span> is the number of points per deformation.</p>
</section>
</section>
<section id="turn-key-points-to-optical-flow">
<h2>Turn Key-points to optical flow<a class="headerlink" href="#turn-key-points-to-optical-flow" title="Permalink to this headline"></a></h2>
<p>Key-points are sparse and lossy representation of what’s going on in the
images. We need to use this information to actually estimate how to
deform the image to do that we do several things:</p>
<ol class="arabic simple">
<li><p><strong>Heatmap Representation</strong>: Turn sparse key-points into a Gaussian
heatmap</p></li>
<li><p><strong>Estimate Image Deformation</strong>: Key-points represent sparse motion,
that is we only know where limited number of points in the source
image should move to. To estimate what happens to the other points in
the image we learn a Thin-Plate Spline Transformation (TPS).</p></li>
<li><p><strong>Occlusion Masks</strong>: Image deformation is too simple for the real
world (e.g. some parts of the driving image can’t be seen on the
source image). We estimate what we don’t know after applying
deformation.</p></li>
</ol>
<p>All this happens inside <a class="reference internal" href="transmotion.html#transmotion.dense_motion.DenseMotionNetwork" title="transmotion.dense_motion.DenseMotionNetwork"><code class="xref py py-class docutils literal notranslate"><span class="pre">transmotion.dense_motion.DenseMotionNetwork</span></code></a> and the output is bundled in <a class="reference internal" href="transmotion.html#transmotion.dense_motion.DenseMotionResult" title="transmotion.dense_motion.DenseMotionResult"><code class="xref py py-class docutils literal notranslate"><span class="pre">transmotion.dense_motion.DenseMotionResult</span></code></a></p>
<section id="tps">
<h3>TPS<a class="headerlink" href="#tps" title="Permalink to this headline"></a></h3>
<p>From a birds eye view, TPS takes a 2D points as input and produces 2D
point as an output. The idea is to make the TPS tell us where points
from the source image should move to match the driving frame. The only
known points for us are the key-points we found earlier. We call those
the control points of the TPS.</p>
<p>TPSs output for the control points on the driving frame will match the
control points on the source frame. Other points will be smoothly moved
based on the movement of the control points (weighted by the distance
from current points to the control points).</p>
<p>The different colors in &#64;fig-key-points-demo represent control points
for different TPS transforms. Here we will use a bunch of the to deform
the image in many different ways. The different TPS transforms are mixed
together using weighting coefficients learned by a neural-net</p>
</section>
<section id="why-do-we-need-occlusion-masks">
<h3>Why do we need occlusion masks?<a class="headerlink" href="#why-do-we-need-occlusion-masks" title="Permalink to this headline"></a></h3>
<p>TPS is a simple transformation compared to the complexity that is
required to transform a pose of a person in a real image. Also, TPS is
locally accurate, that is only fits the key-points we asked it to fit.</p>
<p>Going outside further from those points we lose accuracy completely. We
need to know what are the region we can’t approximate well with a simple
image deformation. This can be because a new feature needs to appear
(e.g. person shows no teeth in the static image, but needs to show those
as part of motion transfer) or because our approximation is bad.</p>
</section>
</section>
<section id="fill-in-the-occluded-areas">
<h2>Fill-in the occluded areas<a class="headerlink" href="#fill-in-the-occluded-areas" title="Permalink to this headline"></a></h2>
<p>At this point we have the optical flow and the occlusion masks, now its
time to inpaint the missing areas from the image. The inpainting is done
by an encoder-decoder architecture. We encode the source image and
decode back the transformed source image. All this happens in <a class="reference internal" href="transmotion.html#transmotion.inpainting.InpaintingNetwork" title="transmotion.inpainting.InpaintingNetwork"><code class="xref py py-class docutils literal notranslate"><span class="pre">transmotion.inpainting.InpaintingNetwork</span></code></a> and the final results are bundled in <a class="reference internal" href="transmotion.html#transmotion.inpainting.InpaintingResult" title="transmotion.inpainting.InpaintingResult"><code class="xref py py-class docutils literal notranslate"><span class="pre">transmotion.inpainting.InpaintingResult</span></code></a></p>
<p>The encoding is a series of feature maps on the decoding end, the
feature maps get deformed by the optical flow and occluded by the
occlusion masks and passed through a decoder layer. So what the
encoder-decoder predicts is the occluded areas. At the end of the
decoder we take a deformed source image, occlude it and add the
predicted occluded areas.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Welcome to TransMotion’s documentation!" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="train_signal.html" class="btn btn-neutral float-right" title="Training Signals" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Gregory Axler.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>